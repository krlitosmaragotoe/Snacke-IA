# ðŸ Snake AI - Aprendizaje por Refuerzo (Deep Q-Learning)

> Un agente autÃ³nomo que aprende a jugar al clÃ¡sico juego "Snake" desde cero utilizando Deep Q-Learning (DQN) y Redes Neuronales con PyTorch.

## ðŸ“‹ DescripciÃ³n

Este proyecto es una implementaciÃ³n de **Reinforcement Learning (RL)** donde una Inteligencia Artificial aprende a jugar Snake sin reglas pre-programadas de movimiento. El agente "nace" sin conocimiento y aprende mediante un sistema de recompensas y castigos, desarrollando estrategias para maximizar su puntuaciÃ³n y evitar colisiones.

El nÃºcleo del aprendizaje se basa en una **Red Neuronal Profunda (Deep Q-Network)** que aproxima la funciÃ³n Q (calidad de una acciÃ³n en un estado dado).

## ðŸ§  Â¿CÃ³mo funciona la IA?

El agente toma decisiones basÃ¡ndose en el estado actual del entorno y recibe retroalimentaciÃ³n inmediata.

### 1. El Vector de Estado (Inputs)

La IA no "ve" la pantalla completa como una imagen. En su lugar, recibe un vector de **11 valores booleanos** (0 o 1) que representan su entorno inmediato:

1.  **Peligro (3 valores):** Â¿Hay pared/cuerpo al frente, a la derecha o a la izquierda?
2.  **DirecciÃ³n actual (4 valores):** Â¿Se estÃ¡ moviendo hacia Oeste, Este, Norte o Sur?
3.  **UbicaciÃ³n de la Comida (4 valores):** Â¿La comida estÃ¡ a la izq, der, arriba o abajo?

### 2. Acciones (Outputs)

La Red Neuronal tiene 3 neuronas de salida (Softmax/Argmax):

- `[1, 0, 0]` -> **Seguir Recto**
- `[0, 1, 0]` -> **Girar Derecha**
- `[0, 0, 1]` -> **Girar Izquierda**

### 3. Sistema de Recompensas (Reinforcement)

- **Comer manzana:** +10 Puntos.
- **Morir (ColisiÃ³n):** -10 Puntos.
- **Moverse (sin comer):** 0 Puntos.

### 4. Arquitectura del Modelo

- **Input Layer:** 11 Neuronas.
- **Hidden Layer:** 256 Neuronas (ActivaciÃ³n ReLU).
- **Output Layer:** 3 Neuronas (Valores Q para las 3 acciones posibles).
- **Optimizador:** Adam.
- **Loss Function:** MSE (Mean Squared Error).

---

## ðŸ“‚ Estructura del Proyecto

```text
snake_ai_project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ agent.py        # Cerebro: Experience Replay y LÃ³gica de entrenamiento
â”‚   â”‚   â””â”€â”€ model.py        # Arquitectura de la Red Neuronal (PyTorch)
â”‚   â”œâ”€â”€ game/
â”‚   â”‚   â””â”€â”€ snake_game_ai.py # Entorno del juego (Pygame) modificado para RL
â”‚   â”œâ”€â”€ main.py             # Entry point y bucle de entrenamiento
â”‚   â”œâ”€â”€ settings.py         # HiperparÃ¡metros (Velocidad, LR, Gamma)
â”‚   â””â”€â”€ utils.py            # Helpers para graficar resultados
â”œâ”€â”€ models/                 # AquÃ­ se guardan los modelos (.pth) entrenados
â”œâ”€â”€ requirements.txt        # Dependencias
â””â”€â”€ README.md
```
